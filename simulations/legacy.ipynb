{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4993321-4bbd-407e-90d1-f6458acc8c0f",
   "metadata": {},
   "source": [
    "## Plotting learning dynamic of task representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c9e1f-9764-4187-94cb-a0d044d3726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000  # Number of epochs\n",
    "dim_input = 2  # Input dimension = (X, Y)\n",
    "dim_hidden = 500  # Hidden neurons\n",
    "dim_output = 1  # Output dimension = Category A or B (0, 1)\n",
    "\n",
    "lr = 2e-3  # Learning rate\n",
    "# Initialization scale\n",
    "# w1_inits = [0.001,0.01,0.1,0.5]\n",
    "w1_inits = [0.001, 0.25]\n",
    "w2_init = 1/dim_hidden\n",
    "\n",
    "# Training\n",
    "\n",
    "models = [LNNet(dim_input, dim_hidden, dim_output, w1_init, w2_init) for w1_init in w1_inits]\n",
    "optimizers = [optim.SGD(model.parameters(), lr=lr) for model in models ]\n",
    "criterions = [nn.MSELoss() for model in models ]\n",
    "\n",
    "model_titles = ['rich', 'lazy']\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    optimizers[m_i].zero_grad()\n",
    "    for m_i, model in enumerate(models):\n",
    "        optimizers[m_i].zero_grad()\n",
    "        y_pred, hiddens = model(X)\n",
    "        loss = criterions[m_i](y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizers[m_i].step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        fig, axes = plt.subplots(2, 2)\n",
    "        fig.set_size_inches(16, 14)\n",
    "        for m_i, model in enumerate(models):\n",
    "            for p_i, (name, param) in enumerate(model.named_parameters()):\n",
    "                axis = axes[m_i, p_i]\n",
    "                if name == 'in_hid.weight':\n",
    "                    in_hid = param.detach().numpy()\n",
    "                    axis.scatter(in_hid[:, 0], in_hid[:, 1])\n",
    "                    axis.set_title(f'in->hid, {model_titles[m_i]}')\n",
    "\n",
    "                elif name == 'hid_out.weight':\n",
    "                    hid_out = param.detach().numpy()\n",
    "                    axis.scatter(np.arange(hid_out.shape[1]), hid_out[0])\n",
    "                    axis.set_title(f'hid->out, {model_titles[m_i]}')\n",
    "\n",
    "        plt.show()\n",
    "        # plt.savefig(f'task_rep_{i:04d}.png')\n",
    "        # plt.cla()\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a79845-f80d-4cba-869f-b0a89afbaaff",
   "metadata": {},
   "source": [
    "## Plotting learning dynamic of task representation (subsampled units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799517d8-366e-4b7b-89b8-9ce218a3954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000  # Number of epochs\n",
    "dim_input = 2  # Input dimension = (X, Y)\n",
    "dim_hidden = 500  # Hidden neurons\n",
    "dim_output = 1  # Output dimension = Category A or B (0, 1)\n",
    "\n",
    "lr = 2e-3  # Learning rate\n",
    "# Initialization scale\n",
    "# w1_inits = [0.001,0.01,0.1,0.5]\n",
    "w1_init = 0.5\n",
    "w2_init = 1/dim_hidden\n",
    "\n",
    "# Training\n",
    "dlnn_model = LNNet(dim_input, dim_hidden, dim_output, w1_init, w2_init)\n",
    "\n",
    "optimizer = optim.SGD(dlnn_model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "vis_num_hidden = 10\n",
    "sub_sample_idx = np.random.choice(dim_hidden, size=vis_num_hidden, replace=False)\n",
    "vis_hidden_color = np.linspace(0, 1, vis_num_hidden)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred, hiddens = dlnn_model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(14, 5)\n",
    "        for p_i, (name, param) in enumerate(dlnn_model.named_parameters()):\n",
    "            axis = axes[p_i]\n",
    "            \n",
    "            if name == 'in_hid.weight':\n",
    "                in_hid = param.detach().numpy()\n",
    "                axis.scatter(in_hid[sub_sample_idx, 0], in_hid[sub_sample_idx, 1], c=vis_hidden_color)\n",
    "                axis.set_title(f'{name}, lazy')\n",
    "                \n",
    "            elif name == 'hid_out.weight':\n",
    "                hid_out = param.detach().numpy()\n",
    "                axis.scatter(sub_sample_idx, hid_out[0, sub_sample_idx], c=vis_hidden_color)\n",
    "                axis.set_title(f'{name}, lazy')\n",
    "        \n",
    "        plt.show()\n",
    "        # plt.savefig(f'lazy_{i:04d}.png')\n",
    "        # plt.cla()\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7fa00-ec8d-4d42-82f7-28c0442d2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob, os\n",
    "# for f in glob.glob(\"task_rep_*.png\"):\n",
    "#     os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb4327-9d42-43f6-a89c-c5b433cdc14d",
   "metadata": {},
   "source": [
    "## Rich-learning via L2-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "205798c5-c176-49e2-883e-23ded4ae6e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "n_epochs = 5000  # Number of epochs\n",
    "dim_input = 2  # Input dimension = (X, Y)\n",
    "dim_hidden = 500  # Hidden neurons\n",
    "dim_output = 1  # Output dimension = Category A or B (0, 1)\n",
    "\n",
    "lr = 2e-3  # Learning rate\n",
    "# Initialization scale\n",
    "# w1_inits = [0.001,0.01,0.1,0.5]\n",
    "w1_init = 0.1\n",
    "w2_init = 1/dim_hidden\n",
    "\n",
    "# wd_lambdas = [0, 0.25, 0.5, 1]\n",
    "wd_lambdas = [0, 0.125, 0.25, 0.5]\n",
    "\n",
    "models = []\n",
    "losses = np.zeros((len(wd_lambdas), n_runs, n_epochs))\n",
    "w1_rel_changes = np.zeros((len(wd_lambdas), n_runs, n_epochs))\n",
    "w2_rel_changes = np.zeros((len(wd_lambdas), n_runs, n_epochs))\n",
    "\n",
    "# Training\n",
    "for w_i, wd_lambda in enumerate(wd_lambdas):\n",
    "  for r_i in range(n_runs):\n",
    "    # Model instantiation\n",
    "    dlnn_model = LNNet(dim_input, dim_hidden, dim_output, w1_init, w2_init)\n",
    "    \n",
    "    loss_per_run, weight_rel_changes_per_run = train(dlnn_model, X, y, n_epochs=n_epochs, lr=lr, wd_lambda=wd_lambda)\n",
    "    losses[w_i, r_i, :] = loss_per_run\n",
    "    w1_rel_changes[w_i, r_i, :] = weight_rel_changes_per_run[0]\n",
    "    w2_rel_changes[w_i, r_i, :] = weight_rel_changes_per_run[1]\n",
    "\n",
    "  models.append(dlnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845ff06-778b-40c9-9910-b715dd3a5c67",
   "metadata": {},
   "source": [
    "## Repeated training via L2-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35af24-7cd2-4e38-a593-f2b6a5096614",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "n_epochs = 5000  # Number of epochs\n",
    "\n",
    "lr = 2e-3  # Learning rate\n",
    "wd_lambda = 0.25\n",
    "\n",
    "bev_bias_models_wd = []\n",
    "bev_bias_losses = np.zeros((2, n_runs, n_epochs))\n",
    "bev_bias_task_losses = np.zeros((2, n_runs, n_epochs))\n",
    "# Training\n",
    "for r_i in range(n_runs):\n",
    "    bev_bias_models_wd.append([])\n",
    "    for m_i, bev_bias_model in enumerate([bev_bias_models[0], bev_bias_models[-1]]):\n",
    "        model = copy.deepcopy(bev_bias_model)\n",
    "        bev_bias_loss_per_run, task_loss_per_run, _ = train_MLP(model, bev_bias_X, bev_bias_y, n_epochs=n_epochs, lr=lr, wd_lambda=wd_lambda)\n",
    "        bev_bias_losses[m_i, r_i, :] = bev_bias_loss_per_run\n",
    "        bev_bias_task_losses[m_i, r_i, :] = task_loss_per_run\n",
    "        bev_bias_models_wd[r_i].append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
